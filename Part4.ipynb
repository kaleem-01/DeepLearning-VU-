{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence,pad_sequence, pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter #Import tensorboard\n",
    "import torch.distributions as dist\n",
    "import numpy\n",
    "import wget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data_rnn import *\n",
    "from elman_rnn import Elman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "#Data is loaded in the form of a list of lists of integers, where each list of integers represents a sentence\n",
    "#xtrain is sorted from short sequences to long sequences\n",
    "\n",
    "#ndfa dataset\n",
    "x_train_ndfa, (i2w_ndfa, w2i_ndfa) = load_ndfa(n=150_000)\n",
    "\n",
    "#brackets dataset\n",
    "x_train_brackets, (i2w_brackets, w2i_brackets) = load_brackets(n=150_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.pad': 0,\n",
       " '.start': 1,\n",
       " '.end': 2,\n",
       " '.unk': 3,\n",
       " 'b': 4,\n",
       " '!': 5,\n",
       " 's': 6,\n",
       " 'a': 7,\n",
       " 'k': 8,\n",
       " 'u': 9,\n",
       " 'w': 10,\n",
       " 'l': 11,\n",
       " 'v': 12,\n",
       " 'c': 13,\n",
       " 'm': 14}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i_ndfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklm!klm!klm!klm!klm!klm!klm!klm!klm!klm!klm!klm!klm!klm!klm!klm!klm!s\n"
     ]
    }
   ],
   "source": [
    "#Decoding a sequence\n",
    "#The code below extracts the characters that correspond to the indices the sequence at position 10_000 of the ndfa dataset\n",
    "print(''.join([i2w_ndfa[i] for i in  x_train_ndfa[149_000]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_ndfa[75000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x27de95f7c90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8kklEQVR4nO3dfVhUdf7/8Rc3ciM6EBogiTdpiSbe5A1NWlmxjkalZaVmRma5+kVL2VX0u4Zmu+tNd1qabrWle311vdnSLUmMMDBX1ERZb1Iql8JWBy2TEVRQ5vz+6MdZZ8UbFESOz8d1nWuZ83nPOZ/3yA6vzpxzxsswDEMAAAAW413bEwAAAKgJhBwAAGBJhBwAAGBJhBwAAGBJhBwAAGBJhBwAAGBJhBwAAGBJhBwAAGBJvrU9gdrkdrt14MABNWzYUF5eXrU9HQAAcBEMw9CxY8cUGRkpb+9zH6+5pkPOgQMHFBUVVdvTAAAAl2D//v1q2rTpOcev6ZDTsGFDSb+8SDabrZZnUwNKSqTIyF9+PnBACgqq3fkAAFANXC6XoqKizL/j53JNh5yKj6hsNps1Q46Pz39+ttkIOQAAS7nQqSaceAwAACyJkAMAACyJkAMAACzpmj4nBwDgyTAMnT59WuXl5bU9FVzDfHx85Ovre9m3dyHkAAAkSWVlZTp48KCOHz9e21MBVL9+fTVp0kR+fn6XvA1CDgBAbrdb+fn58vHxUWRkpPz8/LhJKmqFYRgqKyvT4cOHlZ+fr5tuuum8N/w7H0IOAEBlZWVyu92KiopS/fr1a3s6uMYFBgaqXr16+v7771VWVqaAgIBL2g4nHgMATJf6X8xAdauO30V+mwEAgCURcgAAgCURcgAAgCURcgAAqEUnT57UU089pZiYGPn6+qp///5n1WzYsEE9evRQo0aNFBgYqOjoaL3++usX3Pby5cvVqVMn1a9fX82bN9fLL798Vk1paal+97vfqXnz5vL391eLFi303nvvmePvvPOO7rjjDl133XW67rrrFBcXpy1btpy1nT179ujBBx9UcHCwgoKC1K1bNxUUFJxVZxiG+vbtKy8vL61ateqCPVwOrq4CAKAWlZeXKzAwUM8995w++OCDSmuCgoI0evRodejQQUFBQdqwYYN+/etfKygoSCNGjKj0OWvWrNGQIUP05ptvqnfv3tqzZ4+effZZBQYGavTo0WbdY489psLCQv35z39W69atdfDgQbndbnM8MzNTgwcP1u23366AgADNnDlTvXv31u7du3XDDTdIkvbt26eePXtq+PDhevHFF2Wz2bR79+5Kr4qaPXv2lbs9gXENKyoqMiQZRUVFtT2VmlFcbBjSL0txcW3PBsBV7MSJE8ZXX31lnDhx4j8r3e5f3juu9OJ2V2nud911lzF69Gjj+eefN0JCQoywsDDj7bffNoqLi42nnnrKaNCggdGqVSvjk08+8Xjezp07jT59+hhBQUFGWFiY8cQTTxiHDx82x9esWWP06NHDCA4ONkJDQ434+Hjj22+/Ncfz8/MNScYHH3xg9OrVywgMDDQ6dOhgbNy48dL+EQzDSEhIMPr163dRtQ899JDxxBNPnHN88ODBxiOPPOKx7o033jCaNm1quP//a7xmzRojODjY+Omnny56jqdPnzYaNmxoLFq0yFw3cODA886lwvbt240bbrjBOHjwoCHJWLly5TlrK/2d/P8u9u83H1fVkBYTUytdAKDOOH5catDgyi+XcMflRYsWqXHjxtqyZYvGjBmjUaNG6dFHH9Xtt9+ubdu2qXfv3ho6dKh5N+ejR4/qnnvuUefOnbV161alpaWpsLBQjz32mLnNkpISJSUlaevWrcrIyJC3t7ceeughj6MckvS73/1Ov/3tb5Wbm6ubb75ZgwcP1unTp81xLy8vLVy48NL+Dc5h+/bt2rhxo+66665z1pSWlp51JCUwMFA//PCDvv/+e0nSRx99pK5du2rWrFm64YYbdPPNN+u3v/2tTpw4cc7tHj9+XKdOnVJoaKikX24kmZqaqptvvlkOh0NhYWGKjY0966Oo48eP6/HHH9e8efMUERFxiZ1X0QVjl4XV5JGc5smrK12uKI7kALhIlf5X85nvIVdyqeL71V133WX07NnTfHz69GkjKCjIGDp0qLmu4shBdna2YRiG8dJLLxm9e/f22M7+/fsNSUZeXl6l+zl8+LAhydi5c6dhGP85kvPuu++aNbt37zYkGXv27DHXtWnTxvjwww8vqpcLHcm54YYbDD8/P8Pb29uYNm3aebf1pz/9yahfv77x2WefGeXl5UZeXp4RHR1tSDKPNjkcDsPf39+Ij483Nm/ebKSmphrNmzc3nnrqqXNud9SoUcaNN95o/q5UvLb169c3XnvtNWP79u3G9OnTDS8vLyMzM9N83ogRI4zhw4ebj3UFjuRwTg4AoHL160vFxbWz3yrq0KGD+bOPj48aNWqkmJgYc114eLgk6dChQ5Kkf/7zn/r888/VoEGDs7a1b98+3Xzzzfrmm2+UkpKizZs368cffzSP4BQUFKh9+/aV7rtJkybmfqKjoyVJe/furXI/5/LFF1+ouLhYmzZt0sSJE9W6dWsNHjy40tpnn31W+/bt0/33369Tp07JZrPp+eef19SpU80b7bndbnl5eWnx4sUKDg6WJL322mt65JFH9NZbbykwMNBjmzNmzNDSpUuVmZlpHiWqeF369euncePGSZI6deqkjRs3asGCBbrrrrv00Ucfad26ddq+fXu1vRYXg5ADAKicl5cUFFTbs7go9erV83js5eXlsa7iRNeKP8jFxcV64IEHNHPmzLO2VRFUHnjgATVv3lzvvPOOIiMj5Xa71b59e5WVlZ1z3/+9n+rWsmVLSVJMTIwKCws1derUc4YcLy8vzZw5U3/84x/ldDp1/fXXKyMjQ5J04403Svql1xtuuMEMOJLUtm1bGYahH374QTfddJO5/pVXXtGMGTP02WefeQS7xo0by9fXV+3atfPYf9u2bbVhwwZJ0rp167Rv3z6FhIR41AwYMEB33HGHMjMzL+0FuQBCDgDgmnPrrbfqgw8+UIsWLeTre/afwp9++kl5eXnm5dOSzD/YVwu3263S0tIL1vn4+JhXQf31r3+V3W7X9ddfL0nq0aOHVqxYoeLiYvOo1tdffy1vb281bdrU3MasWbP0hz/8QWvXrlXXrl09tu/n56du3bopLy/PY/3XX3+t5s2bS5ImTpyoZ555xmM8JiZGr7/+uh544IEqdn7xCDkAgGtOYmKi3nnnHQ0ePFgTJkxQaGiovv32Wy1dulTvvvuurrvuOjVq1Ehvv/22mjRpooKCAk2cOPGS9hUdHa3p06froYceOmfNV199pbKyMh05ckTHjh1Tbm6upF8+9pGkefPmqVmzZuZHYOvXr9crr7yi5557ztzG3LlztXLlSvNozY8//qi//e1v6tWrl06ePKn3339fK1asUFZWlvmcxx9/XC+99JKGDRumF198UT/++KPGjx+vp59+2vyoaubMmUpJSdGSJUvUokULOZ1OSVKDBg3MYDR+/HgNHDhQd955p+6++26lpaXp448/No/QREREVHqycbNmzcyjUzWBkAMAuOZERkbqH//4h5KTk9W7d2+VlpaqefPm6tOnj7y9veXl5aWlS5fqueeeU/v27dWmTRu98cYb6tWrV5X3lZeXp6KiovPW3HfffeYVT5LUuXNnSb/cOE/65ajNpEmTlJ+fL19fX7Vq1UozZ87Ur3/9a/M5P/74o/bt2+ex3UWLFum3v/2tDMOQ3W5XZmamunfvbo43aNBA6enpGjNmjLp27apGjRrpscce0+9//3uzZv78+SorK9Mjjzzise0pU6Zo6tSpkqSHHnpICxYs0PTp0/Xcc8+pTZs2+uCDD9SzZ88qvFLVz8uoeAWvQS6XS8HBwSoqKpLNZqvWbZ/rcvHvZsRX637Oq6Tkl8sxpV9OHqwjn60DuPJOnjyp/Px8tWzZstIbuAFX2vl+Jy/27zf3yQEAAJZEyAEAAJZEyAEAAJZEyAEAAJZEyAEAmK7ha1FwlamO30VCDgDAvGvv8Uv4ckygJlT8Lv733ayrgvvkAADk4+OjkJAQ87ud6tevb35FAXAlGYah48eP69ChQwoJCZGPj88lb4uQAwCQJPOOtBVBB6hNISEhld4luSoIOQAASb98oWOTJk0UFhamU6dO1fZ0cA2rV6/eZR3BqUDIAQB48PHxqZY/MEBt48RjAABgSVUKOfPnz1eHDh1ks9lks9lkt9u1Zs0ac7xXr17y8vLyWEaOHOmxjYKCAsXHx6t+/foKCwvT+PHjdfr0aY+azMxM3XrrrfL391fr1q21cOHCs+Yyb948tWjRQgEBAYqNjdWWLVuq0goAALC4KoWcpk2basaMGcrJydHWrVt1zz33qF+/ftq9e7dZ8+yzz+rgwYPmMmvWLHOsvLxc8fHxKisr08aNG7Vo0SItXLhQKSkpZk1+fr7i4+N19913Kzc3V2PHjtUzzzyjtWvXmjXLli1TUlKSpkyZom3btqljx45yOBycLAcAAEyX/S3koaGhevnllzV8+HD16tVLnTp10uzZsyutXbNmje6//34dOHBA4eHhkqQFCxYoOTlZhw8flp+fn5KTk5Wamqpdu3aZzxs0aJCOHj2qtLQ0SVJsbKy6deumuXPnSvrlK+ijoqI0ZswYTZw48aLnzreQAwBQ99T4t5CXl5dr6dKlKikpkd1uN9cvXrxYjRs3Vvv27TVp0iSPG0tlZ2crJibGDDiS5HA45HK5zKNB2dnZiouL89iXw+FQdna2JKmsrEw5OTkeNd7e3oqLizNrzqW0tFQul8tjAQAA1lTlq6t27twpu92ukydPqkGDBlq5cqXatWsnSXr88cfVvHlzRUZGaseOHUpOTlZeXp4+/PBDSZLT6fQIOJLMx06n87w1LpdLJ06c0M8//6zy8vJKa/bu3XveuU+fPl0vvvhiVVsGAAB1UJVDTps2bZSbm6uioiL97W9/U0JCgrKystSuXTuNGDHCrIuJiVGTJk107733at++fWrVqlW1TvxSTJo0SUlJSeZjl8ulqKioWpwRAACoKVUOOX5+fmrdurUkqUuXLvryyy81Z84c/elPfzqrNjY2VpL07bffqlWrVoqIiDjrKqjCwkJJ/7nTZkREhLnuzBqbzabAwEDz/g2V1Vzozoj+/v7y9/evQrcAAKCuuuz75LjdbpWWllY6lpubK0lq0qSJJMlut2vnzp0eV0Glp6fLZrOZH3nZ7XZlZGR4bCc9Pd0878fPz09dunTxqHG73crIyPA4NwgAAFzbqnQkZ9KkSerbt6+aNWumY8eOacmSJcrMzNTatWu1b98+LVmyRPfdd58aNWqkHTt2aNy4cbrzzjvVoUMHSVLv3r3Vrl07DR06VLNmzZLT6dTkyZOVmJhoHmEZOXKk5s6dqwkTJujpp5/WunXrtHz5cqWm/udqpaSkJCUkJKhr167q3r27Zs+erZKSEg0bNqwaXxoAAFCXVSnkHDp0SE8++aQOHjyo4OBgdejQQWvXrtWvfvUr7d+/X5999pkZOKKiojRgwABNnjzZfL6Pj49Wr16tUaNGyW63KygoSAkJCZo2bZpZ07JlS6WmpmrcuHGaM2eOmjZtqnfffVcOh8OsGThwoA4fPqyUlBQ5nU516tRJaWlpZ52MDAAArl2XfZ+cuoz75AAAUPfU+H1yAAAArmaEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYEmEHAAAYElVCjnz589Xhw4dZLPZZLPZZLfbtWbNGnP85MmTSkxMVKNGjdSgQQMNGDBAhYWFHtsoKChQfHy86tevr7CwMI0fP16nT5/2qMnMzNStt94qf39/tW7dWgsXLjxrLvPmzVOLFi0UEBCg2NhYbdmypSqtAAAAi6tSyGnatKlmzJihnJwcbd26Vffcc4/69eun3bt3S5LGjRunjz/+WCtWrFBWVpYOHDighx9+2Hx+eXm54uPjVVZWpo0bN2rRokVauHChUlJSzJr8/HzFx8fr7rvvVm5ursaOHatnnnlGa9euNWuWLVumpKQkTZkyRdu2bVPHjh3lcDh06NChy309AACARXgZhmFczgZCQ0P18ssv65FHHtH111+vJUuW6JFHHpEk7d27V23btlV2drZuu+02rVmzRvfff78OHDig8PBwSdKCBQuUnJysw4cPy8/PT8nJyUpNTdWuXbvMfQwaNEhHjx5VWlqaJCk2NlbdunXT3LlzJUlut1tRUVEaM2aMJk6ceNFzd7lcCg4OVlFRkWw22+W8DGdpMTG10vXfzYiv1v2cV0mJ1KDBLz8XF0tBQVdu3wAA1JCL/ft9yefklJeXa+nSpSopKZHdbldOTo5OnTqluLg4syY6OlrNmjVTdna2JCk7O1sxMTFmwJEkh8Mhl8tlHg3Kzs722EZFTcU2ysrKlJOT41Hj7e2tuLg4s+ZcSktL5XK5PBYAAGBNVQ45O3fuVIMGDeTv76+RI0dq5cqVateunZxOp/z8/BQSEuJRHx4eLqfTKUlyOp0eAadivGLsfDUul0snTpzQjz/+qPLy8kprKrZxLtOnT1dwcLC5REVFVbV9AABQR1Q55LRp00a5ubnavHmzRo0apYSEBH311Vc1MbdqN2nSJBUVFZnL/v37a3tKAACghvhW9Ql+fn5q3bq1JKlLly768ssvNWfOHA0cOFBlZWU6evSox9GcwsJCRURESJIiIiLOugqq4uqrM2v++4qswsJC2Ww2BQYGysfHRz4+PpXWVGzjXPz9/eXv71/VlgEAQB102ffJcbvdKi0tVZcuXVSvXj1lZGSYY3l5eSooKJDdbpck2e127dy50+MqqPT0dNlsNrVr186sOXMbFTUV2/Dz81OXLl08atxutzIyMswaAACAKh3JmTRpkvr27atmzZrp2LFjWrJkiTIzM7V27VoFBwdr+PDhSkpKUmhoqGw2m8aMGSO73a7bbrtNktS7d2+1a9dOQ4cO1axZs+R0OjV58mQlJiaaR1hGjhypuXPnasKECXr66ae1bt06LV++XKmp/7laKSkpSQkJCeratau6d++u2bNnq6SkRMOGDavGlwYAANRlVQo5hw4d0pNPPqmDBw8qODhYHTp00Nq1a/WrX/1KkvT666/L29tbAwYMUGlpqRwOh9566y3z+T4+Plq9erVGjRolu92uoKAgJSQkaNq0aWZNy5YtlZqaqnHjxmnOnDlq2rSp3n33XTkcDrNm4MCBOnz4sFJSUuR0OtWpUyelpaWddTIyAAC4dl32fXLqMu6TAwBA3VPj98kBAAC4mhFyAACAJRFyAACAJRFyAACAJRFyAACAJRFyAACAJRFyAACAJRFyAACAJRFyAACAJVX5W8hR/a6KuyMDAGAxHMkBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWRMgBAACWVKWQM336dHXr1k0NGzZUWFiY+vfvr7y8PI+aXr16ycvLy2MZOXKkR01BQYHi4+NVv359hYWFafz48Tp9+rRHTWZmpm699Vb5+/urdevWWrhw4VnzmTdvnlq0aKGAgADFxsZqy5YtVWkHAABYWJVCTlZWlhITE7Vp0yalp6fr1KlT6t27t0pKSjzqnn32WR08eNBcZs2aZY6Vl5crPj5eZWVl2rhxoxYtWqSFCxcqJSXFrMnPz1d8fLzuvvtu5ebmauzYsXrmmWe0du1as2bZsmVKSkrSlClTtG3bNnXs2FEOh0OHDh261NcCAABYiJdhGMalPvnw4cMKCwtTVlaW7rzzTkm/HMnp1KmTZs+eXelz1qxZo/vvv18HDhxQeHi4JGnBggVKTk7W4cOH5efnp+TkZKWmpmrXrl3m8wYNGqSjR48qLS1NkhQbG6tu3bpp7ty5kiS3262oqCiNGTNGEydOvKj5u1wuBQcHq6ioSDab7VJfhkq1mJha6frvZsRfVm2VlJRIDRr88nNxsRQUdHnbAwDgKnCxf78v65ycoqIiSVJoaKjH+sWLF6tx48Zq3769Jk2apOPHj5tj2dnZiomJMQOOJDkcDrlcLu3evdusiYuL89imw+FQdna2JKmsrEw5OTkeNd7e3oqLizNrKlNaWiqXy+WxAAAAa/K91Ce63W6NHTtWPXr0UPv27c31jz/+uJo3b67IyEjt2LFDycnJysvL04cffihJcjqdHgFHkvnY6XSet8blcunEiRP6+eefVV5eXmnN3r17zznn6dOn68UXX7zUlgEAQB1yySEnMTFRu3bt0oYNGzzWjxgxwvw5JiZGTZo00b333qt9+/apVatWlz7TajBp0iQlJSWZj10ul6KiompxRgAAoKZcUsgZPXq0Vq9erfXr16tp06bnrY2NjZUkffvtt2rVqpUiIiLOugqqsLBQkhQREWH+b8W6M2tsNpsCAwPl4+MjHx+fSmsqtlEZf39/+fv7X1yTAACgTqvSOTmGYWj06NFauXKl1q1bp5YtW17wObm5uZKkJk2aSJLsdrt27tzpcRVUenq6bDab2rVrZ9ZkZGR4bCc9PV12u12S5Ofnpy5dunjUuN1uZWRkmDUAAODaVqUjOYmJiVqyZIn+/ve/q2HDhuY5NMHBwQoMDNS+ffu0ZMkS3XfffWrUqJF27NihcePG6c4771SHDh0kSb1791a7du00dOhQzZo1S06nU5MnT1ZiYqJ5lGXkyJGaO3euJkyYoKefflrr1q3T8uXLlZr6n6uQkpKSlJCQoK5du6p79+6aPXu2SkpKNGzYsOp6bQAAQB1WpZAzf/58Sb9cJn6m999/X0899ZT8/Pz02WefmYEjKipKAwYM0OTJk81aHx8frV69WqNGjZLdbldQUJASEhI0bdo0s6Zly5ZKTU3VuHHjNGfOHDVt2lTvvvuuHA6HWTNw4EAdPnxYKSkpcjqd6tSpk9LS0s46GRkAAFybLus+OXUd98kBAKDuuSL3yQEAALhaEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAl+db2BFB1LSamnrXuuxnxtTATAACuXhzJAQAAlkTIAQAAlkTIAQAAlkTIAQAAlkTIAQAAllSlkDN9+nR169ZNDRs2VFhYmPr376+8vDyPmpMnTyoxMVGNGjVSgwYNNGDAABUWFnrUFBQUKD4+XvXr11dYWJjGjx+v06dPe9RkZmbq1ltvlb+/v1q3bq2FCxeeNZ958+apRYsWCggIUGxsrLZs2VKVdgAAgIVVKeRkZWUpMTFRmzZtUnp6uk6dOqXevXurpKTErBk3bpw+/vhjrVixQllZWTpw4IAefvhhc7y8vFzx8fEqKyvTxo0btWjRIi1cuFApKSlmTX5+vuLj43X33XcrNzdXY8eO1TPPPKO1a9eaNcuWLVNSUpKmTJmibdu2qWPHjnI4HDp06NDlvB4AAMAivAzDMC71yYcPH1ZYWJiysrJ05513qqioSNdff72WLFmiRx55RJK0d+9etW3bVtnZ2brtttu0Zs0a3X///Tpw4IDCw8MlSQsWLFBycrIOHz4sPz8/JScnKzU1Vbt27TL3NWjQIB09elRpaWmSpNjYWHXr1k1z586VJLndbkVFRWnMmDGaOHHiRc3f5XIpODhYRUVFstlsl/oyVKqye9lIld/Ppiq156qvtLakRGrQ4Jefi4uloKBzzBYAgLrjYv9+X9Y5OUVFRZKk0NBQSVJOTo5OnTqluLg4syY6OlrNmjVTdna2JCk7O1sxMTFmwJEkh8Mhl8ul3bt3mzVnbqOipmIbZWVlysnJ8ajx9vZWXFycWVOZ0tJSuVwujwUAAFjTJYcct9utsWPHqkePHmrfvr0kyel0ys/PTyEhIR614eHhcjqdZs2ZAadivGLsfDUul0snTpzQjz/+qPLy8kprKrZRmenTpys4ONhcoqKiqt44AACoEy455CQmJmrXrl1aunRpdc6nRk2aNElFRUXmsn///tqeEgAAqCGX9N1Vo0eP1urVq7V+/Xo1bdrUXB8REaGysjIdPXrU42hOYWGhIiIizJr/vgqq4uqrM2v++4qswsJC2Ww2BQYGysfHRz4+PpXWVGyjMv7+/vL39696wwAAoM6p0pEcwzA0evRorVy5UuvWrVPLli09xrt06aJ69eopIyPDXJeXl6eCggLZ7XZJkt1u186dOz2ugkpPT5fNZlO7du3MmjO3UVFTsQ0/Pz916dLFo8btdisjI8OsAQAA17YqHclJTEzUkiVL9Pe//10NGzY0z38JDg5WYGCggoODNXz4cCUlJSk0NFQ2m01jxoyR3W7XbbfdJknq3bu32rVrp6FDh2rWrFlyOp2aPHmyEhMTzaMsI0eO1Ny5czVhwgQ9/fTTWrdunZYvX67U1P9cVZSUlKSEhAR17dpV3bt31+zZs1VSUqJhw4ZV12sDAADqsCqFnPnz50uSevXq5bH+/fff11NPPSVJev311+Xt7a0BAwaotLRUDodDb731llnr4+Oj1atXa9SoUbLb7QoKClJCQoKmTZtm1rRs2VKpqakaN26c5syZo6ZNm+rdd9+Vw+EwawYOHKjDhw8rJSVFTqdTnTp1Ulpa2lknIwMAgGvTZd0np67jPjkAANQ9V+Q+OQAAAFcrQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAkQg4AALAk39qeAK6Mti+k6YRfgPn4uxnxtTgbAABqHkdyAACAJRFyAACAJRFyAACAJRFyAACAJVU55Kxfv14PPPCAIiMj5eXlpVWrVnmMP/XUU/Ly8vJY+vTp41Fz5MgRDRkyRDabTSEhIRo+fLiKi4s9anbs2KE77rhDAQEBioqK0qxZs86ay4oVKxQdHa2AgADFxMTok08+qWo7AADAoqocckpKStSxY0fNmzfvnDV9+vTRwYMHzeWvf/2rx/iQIUO0e/dupaena/Xq1Vq/fr1GjBhhjrtcLvXu3VvNmzdXTk6OXn75ZU2dOlVvv/22WbNx40YNHjxYw4cP1/bt29W/f3/1799fu3btqmpLAADAgqp8CXnfvn3Vt2/f89b4+/srIiKi0rE9e/YoLS1NX375pbp27SpJevPNN3XffffplVdeUWRkpBYvXqyysjK999578vPz0y233KLc3Fy99tprZhiaM2eO+vTpo/Hjx0uSXnrpJaWnp2vu3LlasGBBVdsCAAAWUyPn5GRmZiosLExt2rTRqFGj9NNPP5lj2dnZCgkJMQOOJMXFxcnb21ubN282a+688075+fmZNQ6HQ3l5efr555/Nmri4OI/9OhwOZWdnn3NepaWlcrlcHgsAALCmag85ffr00V/+8hdlZGRo5syZysrKUt++fVVeXi5JcjqdCgsL83iOr6+vQkND5XQ6zZrw8HCPmorHF6qpGK/M9OnTFRwcbC5RUVGX1ywAALhqVfsdjwcNGmT+HBMTow4dOqhVq1bKzMzUvffeW927q5JJkyYpKSnJfOxyuQg6AABYVI1fQn7jjTeqcePG+vbbbyVJEREROnTokEfN6dOndeTIEfM8noiICBUWFnrUVDy+UM25zgWSfjlXyGazeSwAAMCaajzk/PDDD/rpp5/UpEkTSZLdbtfRo0eVk5Nj1qxbt05ut1uxsbFmzfr163Xq1CmzJj09XW3atNF1111n1mRkZHjsKz09XXa7vaZbAgAAdUCVQ05xcbFyc3OVm5srScrPz1dubq4KCgpUXFys8ePHa9OmTfruu++UkZGhfv36qXXr1nI4HJKktm3bqk+fPnr22We1ZcsW/eMf/9Do0aM1aNAgRUZGSpIef/xx+fn5afjw4dq9e7eWLVumOXPmeHzU9PzzzystLU2vvvqq9u7dq6lTp2rr1q0aPXp0NbwsAACgrqtyyNm6das6d+6szp07S5KSkpLUuXNnpaSkyMfHRzt27NCDDz6om2++WcOHD1eXLl30xRdfyN/f39zG4sWLFR0drXvvvVf33Xefevbs6XEPnODgYH366afKz89Xly5d9Jvf/EYpKSke99K5/fbbtWTJEr399tvq2LGj/va3v2nVqlVq37795bweAADAIqp84nGvXr1kGMY5x9euXXvBbYSGhmrJkiXnrenQoYO++OKL89Y8+uijevTRRy+4PwAAcO3hu6sAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAlEXIAAIAl+db2BHD1aDExtdL1382Iv8IzAQDg8nEkBwAAWBIhBwAAWBIhBwAAWBIhBwAAWBIhBwAAWBIhBwAAWBIhBwAAWBIhBwAAWBIhBwAAWBIhBwAAWBIhBwAAWBIhBwAAWFKVQ8769ev1wAMPKDIyUl5eXlq1apXHuGEYSklJUZMmTRQYGKi4uDh98803HjVHjhzRkCFDZLPZFBISouHDh6u4uNijZseOHbrjjjsUEBCgqKgozZo166y5rFixQtHR0QoICFBMTIw++eSTqrYDAAAsqsohp6SkRB07dtS8efMqHZ81a5beeOMNLViwQJs3b1ZQUJAcDodOnjxp1gwZMkS7d+9Wenq6Vq9erfXr12vEiBHmuMvlUu/evdW8eXPl5OTo5Zdf1tSpU/X222+bNRs3btTgwYM1fPhwbd++Xf3791f//v21a9euqrYEAAAsyLeqT+jbt6/69u1b6ZhhGJo9e7YmT56sfv36SZL+8pe/KDw8XKtWrdKgQYO0Z88epaWl6csvv1TXrl0lSW+++abuu+8+vfLKK4qMjNTixYtVVlam9957T35+frrllluUm5ur1157zQxDc+bMUZ8+fTR+/HhJ0ksvvaT09HTNnTtXCxYsuKQXAwAAWEe1npOTn58vp9OpuLg4c11wcLBiY2OVnZ0tScrOzlZISIgZcCQpLi5O3t7e2rx5s1lz5513ys/Pz6xxOBzKy8vTzz//bNacuZ+Kmor9VKa0tFQul8tjAQAA1lStIcfpdEqSwsPDPdaHh4ebY06nU2FhYR7jvr6+Cg0N9aipbBtn7uNcNRXjlZk+fbqCg4PNJSoqqqotAgCAOuKaurpq0qRJKioqMpf9+/fX9pQAAEANqdaQExERIUkqLCz0WF9YWGiORURE6NChQx7jp0+f1pEjRzxqKtvGmfs4V03FeGX8/f1ls9k8FgAAYE3VGnJatmypiIgIZWRkmOtcLpc2b94su90uSbLb7Tp69KhycnLMmnXr1sntdis2NtasWb9+vU6dOmXWpKenq02bNrruuuvMmjP3U1FTsR8AAHBtq3LIKS4uVm5urnJzcyX9crJxbm6uCgoK5OXlpbFjx+r3v/+9PvroI+3cuVNPPvmkIiMj1b9/f0lS27Zt1adPHz377LPasmWL/vGPf2j06NEaNGiQIiMjJUmPP/64/Pz8NHz4cO3evVvLli3TnDlzlJSUZM7j+eefV1paml599VXt3btXU6dO1datWzV69OjLf1UAAECdV+VLyLdu3aq7777bfFwRPBISErRw4UJNmDBBJSUlGjFihI4ePaqePXsqLS1NAQEB5nMWL16s0aNH695775W3t7cGDBigN954wxwPDg7Wp59+qsTERHXp0kWNGzdWSkqKx710br/9di1ZskSTJ0/W//7v/+qmm27SqlWr1L59+0t6IQAAgLVUOeT06tVLhmGcc9zLy0vTpk3TtGnTzlkTGhqqJUuWnHc/HTp00BdffHHemkcffVSPPvro+ScMAACuSdfU1VUAAODaQcgBAACWRMgBAACWVOVzcoAKLSamnrXuuxnxtTATAADOxpEcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSYQcAABgSb61PQFcG1pMTD1r3Xcz4mthJgCAawVHcgAAgCURcgAAgCURcgAAgCURcgAAgCURcgAAgCURcgAAgCURcgAAgCURcgAAgCURcgAAgCURcgAAgCVVe8iZOnWqvLy8PJbo6Ghz/OTJk0pMTFSjRo3UoEEDDRgwQIWFhR7bKCgoUHx8vOrXr6+wsDCNHz9ep0+f9qjJzMzUrbfeKn9/f7Vu3VoLFy6s7lYAAEAdViNHcm655RYdPHjQXDZs2GCOjRs3Th9//LFWrFihrKwsHThwQA8//LA5Xl5ervj4eJWVlWnjxo1atGiRFi5cqJSUFLMmPz9f8fHxuvvuu5Wbm6uxY8fqmWee0dq1a2uiHQAAUAfVyBd0+vr6KiIi4qz1RUVF+vOf/6wlS5bonnvukSS9//77atu2rTZt2qTbbrtNn376qb766it99tlnCg8PV6dOnfTSSy8pOTlZU6dOlZ+fnxYsWKCWLVvq1VdflSS1bdtWGzZs0Ouvvy6Hw1ETLQEAgDqmRo7kfPPNN4qMjNSNN96oIUOGqKCgQJKUk5OjU6dOKS4uzqyNjo5Ws2bNlJ2dLUnKzs5WTEyMwsPDzRqHwyGXy6Xdu3ebNWduo6KmYhvnUlpaKpfL5bEAAABrqvYjObGxsVq4cKHatGmjgwcP6sUXX9Qdd9yhXbt2yel0ys/PTyEhIR7PCQ8Pl9PplCQ5nU6PgFMxXjF2vhqXy6UTJ04oMDCw0rlNnz5dL774YnW0iRrUYmJqpeu/mxF/hWcCAKjLqj3k9O3b1/y5Q4cOio2NVfPmzbV8+fJzho8rZdKkSUpKSjIfu1wuRUVF1eKMAABATanxS8hDQkJ0880369tvv1VERITKysp09OhRj5rCwkLzHJ6IiIizrraqeHyhGpvNdt4g5e/vL5vN5rEAAABrqvGQU1xcrH379qlJkybq0qWL6tWrp4yMDHM8Ly9PBQUFstvtkiS73a6dO3fq0KFDZk16erpsNpvatWtn1py5jYqaim0AAABUe8j57W9/q6ysLH333XfauHGjHnroIfn4+Gjw4MEKDg7W8OHDlZSUpM8//1w5OTkaNmyY7Ha7brvtNklS79691a5dOw0dOlT//Oc/tXbtWk2ePFmJiYny9/eXJI0cOVL/+te/NGHCBO3du1dvvfWWli9frnHjxlV3OwAAoI6q9nNyfvjhBw0ePFg//fSTrr/+evXs2VObNm3S9ddfL0l6/fXX5e3trQEDBqi0tFQOh0NvvfWW+XwfHx+tXr1ao0aNkt1uV1BQkBISEjRt2jSzpmXLlkpNTdW4ceM0Z84cNW3aVO+++y6XjwMAAFO1h5ylS5eedzwgIEDz5s3TvHnzzlnTvHlzffLJJ+fdTq9evbR9+/ZLmiMAALA+vrsKAABYEiEHAABYEiEHAABYEiEHAABYEiEHAABYUo18CzlwpfA9VwCAc+FIDgAAsCRCDgAAsCRCDgAAsCRCDgAAsCRCDgAAsCRCDgAAsCRCDgAAsCTuk4NrSmX31eGeOgBgTRzJAQAAlkTIAQAAlkTIAQAAlkTIAQAAlkTIAQAAlkTIAQAAlkTIAQAAlsR9coBzqOyeOhL31QGAuoIjOQAAwJIIOQAAwJIIOQAAwJIIOQAAwJI48RioBpykDABXH47kAAAASyLkAAAAS+LjKuAK46MtALgyOJIDAAAsiZADAAAsiY+rgKtcZR9v8dEWAFwYR3IAAIAlcSQHsBBOagaA/+BIDgAAsKQ6fyRn3rx5evnll+V0OtWxY0e9+eab6t69e21PC7jqcdQHgNXV6ZCzbNkyJSUlacGCBYqNjdXs2bPlcDiUl5ensLCw2p4eYCmcAA2grqnTIee1117Ts88+q2HDhkmSFixYoNTUVL333nuaOHFiLc8OuHZVJRBxRAlATamzIaesrEw5OTmaNGmSuc7b21txcXHKzs6u9DmlpaUqLS01HxcVFUmSXC5Xtc/PXXq80vWV7asqteeqr7S2pMT8sbz0uNyGu3bmUQ2156q/0nOujnnUxTlXxzyqY87tp6yttHbXi45K11emqtuorL4q+wNQ/SreHwzDOH+hUUf9+9//NiQZGzdu9Fg/fvx4o3v37pU+Z8qUKYYkFhYWFhYWFgss+/fvP29WqLNHci7FpEmTlJSUZD52u906cuSIGjVqJC8vrypvz+VyKSoqSvv375fNZqvOqV5V6NNa6NM6roUeJfq0muro0zAMHTt2TJGRkeetq7Mhp3HjxvLx8VFhYaHH+sLCQkVERFT6HH9/f/n7+3usCwkJuey52Gw2S/9CVqBPa6FP67gWepTo02out8/g4OAL1tTZ++T4+fmpS5cuysjIMNe53W5lZGTIbrfX4swAAMDVoM4eyZGkpKQkJSQkqGvXrurevbtmz56tkpIS82orAABw7arTIWfgwIE6fPiwUlJS5HQ61alTJ6WlpSk8PPyK7N/f319Tpkw56yMwq6FPa6FP67gWepTo02quZJ9ehnGh668AAADqnjp7Tg4AAMD5EHIAAIAlEXIAAIAlEXIAAIAlEXIuw7x589SiRQsFBAQoNjZWW7Zsqe0pXbLp06erW7duatiwocLCwtS/f3/l5eV51Jw8eVKJiYlq1KiRGjRooAEDBpx1M8a6ZsaMGfLy8tLYsWPNdVbp89///reeeOIJNWrUSIGBgYqJidHWrVvNccMwlJKSoiZNmigwMFBxcXH65ptvanHGVVdeXq4XXnhBLVu2VGBgoFq1aqWXXnrJ4/ts6mKf69ev1wMPPKDIyEh5eXlp1apVHuMX09ORI0c0ZMgQ2Ww2hYSEaPjw4SouLr6CXVzY+fo8deqUkpOTFRMTo6CgIEVGRurJJ5/UgQMHPLZxtfd5oX/LM40cOVJeXl6aPXu2x/qrvUfp4vrcs2ePHnzwQQUHBysoKEjdunVTQUGBOV4T772EnEu0bNkyJSUlacqUKdq2bZs6duwoh8OhQ4cO1fbULklWVpYSExO1adMmpaen69SpU+rdu7dKzviSz3Hjxunjjz/WihUrlJWVpQMHDujhhx+uxVlfni+//FJ/+tOf1KFDB4/1Vujz559/Vo8ePVSvXj2tWbNGX331lV599VVdd911Zs2sWbP0xhtvaMGCBdq8ebOCgoLkcDh08uTJWpx51cycOVPz58/X3LlztWfPHs2cOVOzZs3Sm2++adbUxT5LSkrUsWNHzZs3r9Lxi+lpyJAh2r17t9LT07V69WqtX79eI0aMuFItXJTz9Xn8+HFt27ZNL7zwgrZt26YPP/xQeXl5evDBBz3qrvY+L/RvWWHlypXatGlTpV9TcLX3KF24z3379qlnz56Kjo5WZmamduzYoRdeeEEBAQFmTY28917+V2Vem7p3724kJiaaj8vLy43IyEhj+vTptTir6nPo0CFDkpGVlWUYhmEcPXrUqFevnrFixQqzZs+ePYYkIzs7u7amecmOHTtm3HTTTUZ6erpx1113Gc8//7xhGNbpMzk52ejZs+c5x91utxEREWG8/PLL5rqjR48a/v7+xl//+tcrMcVqER8fbzz99NMe6x5++GFjyJAhhmFYo09JxsqVK83HF9PTV199ZUgyvvzyS7NmzZo1hpeXl/Hvf//7is29Kv67z8ps2bLFkGR8//33hmHUvT7P1eMPP/xg3HDDDcauXbuM5s2bG6+//ro5Vtd6NIzK+xw4cKDxxBNPnPM5NfXey5GcS1BWVqacnBzFxcWZ67y9vRUXF6fs7OxanFn1KSoqkiSFhoZKknJycnTq1CmPnqOjo9WsWbM62XNiYqLi4+M9+pGs0+dHH32krl276tFHH1VYWJg6d+6sd955xxzPz8+X0+n06DM4OFixsbF1qs/bb79dGRkZ+vrrryVJ//znP7Vhwwb17dtXknX6PNPF9JSdna2QkBB17drVrImLi5O3t7c2b958xedcXYqKiuTl5WV+56AV+nS73Ro6dKjGjx+vW2655axxq/SYmpqqm2++WQ6HQ2FhYYqNjfX4SKum3nsJOZfgxx9/VHl5+Vl3Vg4PD5fT6aylWVUft9utsWPHqkePHmrfvr0kyel0ys/P76wvNK2LPS9dulTbtm3T9OnTzxqzSp//+te/NH/+fN10001au3atRo0apeeee06LFi2SJLOXuv47PHHiRA0aNEjR0dGqV6+eOnfurLFjx2rIkCGSrNPnmS6mJ6fTqbCwMI9xX19fhYaG1tm+T548qeTkZA0ePNj8Ukcr9Dlz5kz5+vrqueeeq3TcCj0eOnRIxcXFmjFjhvr06aNPP/1UDz30kB5++GFlZWVJqrn33jr9tQ6oGYmJidq1a5c2bNhQ21Opdvv379fzzz+v9PR0j8+Crcbtdqtr16764x//KEnq3Lmzdu3apQULFighIaGWZ1d9li9frsWLF2vJkiW65ZZblJubq7FjxyoyMtJSfV7rTp06pccee0yGYWj+/Pm1PZ1qk5OTozlz5mjbtm3y8vKq7enUGLfbLUnq16+fxo0bJ0nq1KmTNm7cqAULFuiuu+6qsX1zJOcSNG7cWD4+Pmed9V1YWKiIiIhamlX1GD16tFavXq3PP/9cTZs2NddHRESorKxMR48e9aivaz3n5OTo0KFDuvXWW+Xr6ytfX19lZWXpjTfekK+vr8LDwy3RZ5MmTdSuXTuPdW3btjWvZKjopa7/Do8fP948mhMTE6OhQ4dq3Lhx5lE6q/R5povpKSIi4qyLIE6fPq0jR47Uub4rAs7333+v9PR08yiOVPf7/OKLL3To0CE1a9bMfD/6/vvv9Zvf/EYtWrSQVPd7lH75m+nr63vB96SaeO8l5FwCPz8/denSRRkZGeY6t9utjIwM2e32WpzZpTMMQ6NHj9bKlSu1bt06tWzZ0mO8S5cuqlevnkfPeXl5KigoqFM933vvvdq5c6dyc3PNpWvXrhoyZIj5sxX67NGjx1m3APj666/VvHlzSVLLli0VERHh0afL5dLmzZvrVJ/Hjx+Xt7fn25iPj4/5X45W6fNMF9OT3W7X0aNHlZOTY9asW7dObrdbsbGxV3zOl6oi4HzzzTf67LPP1KhRI4/xut7n0KFDtWPHDo/3o8jISI0fP15r166VVPd7lH75m9mtW7fzvifV2N+YSz5l+Rq3dOlSw9/f31i4cKHx1VdfGSNGjDBCQkIMp9NZ21O7JKNGjTKCg4ONzMxM4+DBg+Zy/Phxs2bkyJFGs2bNjHXr1hlbt2417Ha7Ybfba3HW1ePMq6sMwxp9btmyxfD19TX+8Ic/GN98842xePFio379+sb//d//mTUzZswwQkJCjL///e/Gjh07jH79+hktW7Y0Tpw4UYszr5qEhATjhhtuMFavXm3k5+cbH374odG4cWNjwoQJZk1d7PPYsWPG9u3bje3btxuSjNdee83Yvn27eVXRxfTUp08fo3PnzsbmzZuNDRs2GDfddJMxePDg2mqpUufrs6yszHjwwQeNpk2bGrm5uR7vS6WlpeY2rvY+L/Rv+d/+++oqw7j6ezSMC/f54YcfGvXq1TPefvtt45tvvjHefPNNw8fHx/jiiy/MbdTEey8h5zK8+eabRrNmzQw/Pz+je/fuxqZNm2p7SpdMUqXL+++/b9acOHHC+J//+R/juuuuM+rXr2889NBDxsGDB2tv0tXkv0OOVfr8+OOPjfbt2xv+/v5GdHS08fbbb3uMu91u44UXXjDCw8MNf39/49577zXy8vJqabaXxuVyGc8//7zRrFkzIyAgwLjxxhuN3/3udx5/BOtin59//nml/39MSEgwDOPievrpp5+MwYMHGw0aNDBsNpsxbNgw49ixY7XQzbmdr8/8/Pxzvi99/vnn5jau9j4v9G/53yoLOVd7j4ZxcX3++c9/Nlq3bm0EBAQYHTt2NFatWuWxjZp47/UyjDNuDQoAAGARnJMDAAAsiZADAAAsiZADAAAsiZADAAAsiZADAAAsiZADAAAsiZADAAAsiZADAAAsiZADAAAsiZADAAAsiZADAAAsiZADAAAs6f8Bi2psulS0AJcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot distribution of batch sizes for the ndfa dataset\n",
    "mean = numpy.mean([len(x) for x in x_train_ndfa])\n",
    "plt.hist([len(x) for x in x_train_ndfa], bins=80)\n",
    "\n",
    "#plot mean\n",
    "plt.axvline(mean, color='r', label=f'mean: {mean}' )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21045\n"
     ]
    }
   ],
   "source": [
    "#Count the number of sequences of length 10 in the ndfa dataset\n",
    "print(sum([len(x) == 10 for x in x_train_ndfa]))\n",
    "\n",
    "#Most sequences are of a shorter length than 50!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching and padding\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(sequences, max_tokens):\n",
    "\n",
    "    start = 0\n",
    "    end = max_tokens\n",
    "    step = 10\n",
    "\n",
    "    # Dynamically create ranges\n",
    "    ranges = [(i, i + step) for i in range(start, end, step)]\n",
    "    #Create a dictionary with the keys being the sequence length ranges and the values being the sequences that fall within that range\n",
    "    batches_dictionary = {range_tuple: [] for range_tuple in ranges}\n",
    "\n",
    "    for range_tuple in ranges:\n",
    "        start, end = range_tuple\n",
    "        for sequence in sequences:\n",
    "            if len(sequence) > start and len(sequence) <= end:\n",
    "                batches_dictionary[range_tuple].append(sequence)\n",
    "\n",
    "    return batches_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_list = [1]\n",
    "# sequences = [[1,2,3],[0,2,3,4],[0,2,3,4],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,1,1,1,1]]\n",
    "\n",
    "\n",
    "# start = 0\n",
    "# end = 30\n",
    "# step = 10\n",
    "\n",
    "# # Dynamically create ranges\n",
    "# ranges = [(i, i + step) for i in range(start, end, step)]\n",
    "# #Create a dictionary with the keys being the sequence length ranges and the values being the sequences that fall within that range\n",
    "# sequence_length_dict = {range_tuple: [] for range_tuple in ranges}\n",
    "\n",
    "# for range_tuple in ranges:\n",
    "#     start, end = range_tuple\n",
    "#     for sequence in sequences:\n",
    "#         if len(sequence) > start and len(sequence) <= end:\n",
    "#             sequence_length_dict[range_tuple].append(sequence)\n",
    "\n",
    "# sequence_length_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " [6, 6],\n",
       " ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = len(x_train_ndfa[-1])\n",
    "batches_dictionary = create_batches(x_train_ndfa, max_tokens)\n",
    "batches_dictionary[(0, 10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append start and end tokens to the sequences\n",
    "\n",
    "def add_start_end_tokens(batches_dictionary, start_token = 1, end_token = 2):\n",
    "\n",
    "    batches_dictionary_with_start_end_tokens = {}\n",
    "    \n",
    "    for key in batches_dictionary.keys():\n",
    "        sequences = batches_dictionary[key] #Get the sequences that fall within the range given by the key\n",
    "        sequences_with_start_end_tokens = []\n",
    "        for sequence in sequences:\n",
    "            sequence_with_start_end_tokens = [start_token] + sequence + [end_token]\n",
    "            sequences_with_start_end_tokens.append(sequence_with_start_end_tokens)\n",
    "        batches_dictionary_with_start_end_tokens[key] = sequences_with_start_end_tokens\n",
    "    \n",
    "    return batches_dictionary_with_start_end_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_dictionary_with_start_end_tokens = add_start_end_tokens(batches_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad sequences to the same length\n",
    "\n",
    "def pad_sequences(batches_dictionary_with_start_end_tokens):\n",
    "\n",
    "    batches_dictionary_padded = {}\n",
    "\n",
    "    for key in batches_dictionary_with_start_end_tokens.keys():\n",
    "        sequences = batches_dictionary_with_start_end_tokens[key]\n",
    "        \n",
    "        #Convert the sequences to tensors and pad them\n",
    "        padded_sequences = pad_sequence([torch.tensor(sequence) for sequence in sequences], batch_first=True, padding_value=0)\n",
    "        batches_dictionary_padded[key] = padded_sequences\n",
    "\n",
    "    return batches_dictionary_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_dictionary_padded = pad_sequences(batches_dictionary_with_start_end_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([86723, 12])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display shape of the padded sequences in the form (batch_size, sequence_length), denoted as (batch, time) in the pytorch\n",
    "batches_dictionary_padded[(0, 10)].shape\n",
    "\n",
    "#NOTE: This is a dictionary of tensors now. Since batch first equals true, the shape you see below indicates that there are 86723 sequences in this batch, where each sequence has a length of 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create target tensors \n",
    "def create_target_tensors(batches_dictionary_padded):\n",
    "\n",
    "    batches_dictionary_target_tensors = {}\n",
    "\n",
    "    for key in batches_dictionary_padded.keys():\n",
    "        batch_tensor = batches_dictionary_padded[key]\n",
    "        shifted_tensor = batch_tensor[:, 1:] # Remove the <start> token (first column)\n",
    "        zero_column = torch.zeros((batch_tensor.size(0), 1), dtype=batch_tensor.dtype)  # Define a tensor of zeros with the same size and dtype as the batch_tensor\n",
    "        target_tensor = torch.cat((shifted_tensor, zero_column), dim=1)\n",
    "        batches_dictionary_target_tensors[key] = target_tensor\n",
    "\n",
    "    return batches_dictionary_target_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_dictionary_target_tensors = create_target_tensors(batches_dictionary_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([86723, 12])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_dictionary_target_tensors[(0, 10)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up autoregressive model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoregressive_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,embedding_size, vocab_size, num_layers_lstm, hidden_size_lstm):\n",
    "\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        - embedding_size: The size of the embeddings (size of each x_t vector)\n",
    "        - vocab_size: The size of the vocabulary (number of unique words/tokens)\n",
    "        - num_layers_lstm: The number of layers in the LSTM (number of stacked LSTMs)\n",
    "        - hidden_size_lstm: The hidden state size of the LSTM (size of the h_t vector)\n",
    "        \"\"\"\n",
    "\n",
    "        super(autoregressive_model, self).__init__() #Call the constructor of the parent class (torch.nn.Module) -- > This gives us methods like model.parameters() \n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers_lstm = num_layers_lstm\n",
    "        self.hidden_size_lstm = hidden_size_lstm\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "        self.lstm = torch.nn.LSTM(input_size = embedding_size, hidden_size = hidden_size_lstm, num_layers = num_layers_lstm, batch_first = True)\n",
    "        self.linear = torch.nn.Linear(in_features = hidden_size_lstm, out_features = vocab_size) #Output size is the same as the vocab size since we are predicting probabilities for each of the words/tokens in the vocabulary\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        - x: A tensor of shape (batch_size, sequence_length) containing the input sequences\n",
    "        \"\"\"\n",
    "\n",
    "        #NOTE: the input tensor x will have different batche sizes and sequence lengths (as we set it up like that earlier)\n",
    "\n",
    "        #Embed the input sequences\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        #Pass the embeddings through the LSTM\n",
    "        lstm_output, _ = self.lstm(embedded) #We only need the output from the LSTM, no need for hn and cn\n",
    "\n",
    "        #Pass the LSTM output through the linear layer\n",
    "        output = self.linear(lstm_output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elman RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class elman_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,embedding_size, vocab_size, hidden_size):\n",
    "\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        - embedding_size: The size of the embeddings (size of each x_t vector)\n",
    "        - vocab_size: The size of the vocabulary (number of unique words/tokens)\n",
    "        - num_layers_lstm: The number of layers in the LSTM (number of stacked LSTMs)\n",
    "        - hidden_size_lstm: The hidden state size of the LSTM (size of the h_t vector)\n",
    "        \"\"\"\n",
    "\n",
    "        super(elman_model, self).__init__() #Call the constructor of the parent class (torch.nn.Module) -- > This gives us methods like model.parameters() \n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        # self.num_layers_lstm = num_layers_lstm\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "        # self.lstm = torch.nn.LSTM(input_size = embedding_size, hidden_size = hidden_size_lstm, num_layers = num_layers_lstm, batch_first = True)\n",
    "        self.elman = Elman(insize=embedding_size, outsize=hidden_size, hsize=hidden_size)\n",
    "        self.linear = torch.nn.Linear(in_features = hidden_size, out_features = vocab_size) #Output size is the same as the vocab size since we are predicting probabilities for each of the words/tokens in the vocabulary\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        - x: A tensor of shape (batch_size, sequence_length) containing the input sequences\n",
    "        \"\"\"\n",
    "\n",
    "        #NOTE: the input tensor x will have different batche sizes and sequence lengths (as we set it up like that earlier)\n",
    "\n",
    "        #Embed the input sequences\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        #Pass the embeddings through the LSTM\n",
    "        elman_output, _ = self.elman(embedded) #We only need the output from the LSTM, no need for hn and cn\n",
    "\n",
    "        #Pass the LSTM output through the linear layer\n",
    "        output = self.linear(elman_output)\n",
    "\n",
    "        return output\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {1:2, 3:4}\n",
    "\n",
    "dict_key_list = list(dict.keys())\n",
    "\n",
    "np.random.shuffle(dict_key_list)\n",
    "\n",
    "dict_key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(model, batches_dictionary_padded, batches_dictionary_target_tensors, optimizer, loss_fn, num_epochs):\n",
    "\n",
    "    #tb_writer = SummaryWriter(log_dir='runs/experiment1') #Create a tensorboard writer object\n",
    "\n",
    "    batch_dictionary_keys = list(batches_dictionary_padded.keys())\n",
    "    model.train() #Set the model to training mode (enables dropout and batch normalization)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(f'Epoch {epoch}')\n",
    "\n",
    "        running_loss = 0.\n",
    "        last_loss = 0\n",
    "\n",
    "        #Shuffle the keys to get randomly ordered batches for each epoch (Esnures LSTM does not forget what is has learnt from the long sequences)\n",
    "        np.random.shuffle(batch_dictionary_keys)\n",
    "\n",
    "        for i,key in enumerate(batch_dictionary_keys):\n",
    "            \n",
    "            #Get the input and target tensors\n",
    "            input_tensor = batches_dictionary_padded[key]\n",
    "            target_tensor = batches_dictionary_target_tensors[key]\n",
    "\n",
    "            #reshape the target tensor to be of shape (batch_size*sequence_length)\n",
    "            target_tensor_reshaped = target_tensor.reshape(-1)\n",
    "\n",
    "            output = model.forward(input_tensor)\n",
    "\n",
    "            #Reshape the output tensor to be of shape (batch_size*sequence_length, vocab_size)\n",
    "            output_reshaped = output.reshape(-1, model.vocab_size)\n",
    "\n",
    "            #Calculate the loss\n",
    "\n",
    "            loss = loss_fn(output_reshaped, target_tensor_reshaped) #Loss is automatically averaged over the batch size by default\n",
    "            \n",
    "            #Backpropagate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Update the weights\n",
    "            optimizer.step()    \n",
    "\n",
    "            optimizer.zero_grad() #Reset the gradients after update so they don't accumulate over epochs\n",
    "            \n",
    "            #Determine the average loss after processing every batch in the dictionary\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i % len(batch_dictionary_keys) == len(batch_dictionary_keys) - 1:    # print every 16 batches\n",
    "                last_loss = running_loss / len(batch_dictionary_keys) # Average loss over the batches in the dictionary\n",
    "                print(\"Average loss:\",last_loss)\n",
    "                #tb_x = epoch * len(batch_dictionary_keys) + i + 1\n",
    "                #tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "                running_loss = 0.\n",
    "\n",
    "    #return last_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(15 % 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### NDFA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data\n",
    "#----------------\n",
    "max_tokens = len(x_train_ndfa[-1])\n",
    "batches_dictionary_ndfa = create_batches(x_train_ndfa, max_tokens)\n",
    "batches_dictionary_ndfa_with_start_end_tokens = add_start_end_tokens(batches_dictionary_ndfa)\n",
    "batches_dictionary_ndfa_padded = pad_sequences(batches_dictionary_ndfa_with_start_end_tokens)\n",
    "batches_dictionary_target_tensors = create_target_tensors(batches_dictionary_ndfa_padded)\n",
    "\n",
    "#Create model\n",
    "#----------------\n",
    "autoregressive_nn = autoregressive_model(embedding_size=32, vocab_size=len(w2i_ndfa), num_layers_lstm=1, hidden_size_lstm=16)\n",
    "elman_nn = elman_model(embedding_size=300, vocab_size=len(w2i_ndfa), hidden_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Average loss: 0.7755760680884123\n"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "# optimizer = torch.optim.Adam(autoregressive_nn.parameters(), lr=0.01) \n",
    "optimizer = torch.optim.Adam(elman_nn.parameters(), lr=0.01) \n",
    "loss_fn = torch.nn.CrossEntropyLoss() #CrossEntropyLoss expects the input to be of shape (batch_size, num_classes) and the target to be of shape (batch_size)\n",
    "num_epochs = 1\n",
    "\n",
    "# train_loop(autoregressive_nn, batches_dictionary_ndfa_padded, batches_dictionary_target_tensors, optimizer, loss_fn, num_epochs)\n",
    "train_loop(elman_nn, batches_dictionary_ndfa_padded, batches_dictionary_target_tensors, optimizer, loss_fn, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes (for personal learning)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Understanding pytorch tensor shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7012, -1.2353, -0.7167, -1.1609,  1.2597,  1.3311],\n",
       "         [-0.7936, -0.9851,  1.2126, -0.5823,  1.0790,  0.7350],\n",
       "         [ 0.1162,  1.2397,  0.3916,  1.1953,  1.6666,  0.6235]],\n",
       "\n",
       "        [[ 1.3959, -2.2150,  0.3461,  1.1509,  1.3529,  1.6268],\n",
       "         [ 0.4229,  0.5596, -0.2371, -0.0978, -0.9853, -0.4916],\n",
       "         [-0.7329,  1.2261, -1.7137,  0.6930, -0.0240,  0.8877]],\n",
       "\n",
       "        [[ 1.0693, -1.8661, -0.5422,  0.0634,  0.3816, -0.7339],\n",
       "         [ 1.1769, -0.7760,  0.9853,  0.9291, -0.4743, -0.1829],\n",
       "         [ 0.2282,  0.6171,  0.8953,  1.4716,  0.5739, -1.0707]],\n",
       "\n",
       "        [[ 1.5296,  0.5418,  0.5715,  0.2332, -0.0703,  1.3368],\n",
       "         [-0.6625,  1.7817,  0.3325,  0.1132,  0.5180,  1.7247],\n",
       "         [ 1.8628, -1.9144,  1.0901, -2.2404, -1.1076,  0.2678]],\n",
       "\n",
       "        [[ 0.7804,  0.5619,  0.0783,  0.2088, -0.2959,  1.0336],\n",
       "         [ 0.8405, -0.1232, -1.1853,  1.2130, -0.4039,  0.6148],\n",
       "         [ 1.2946,  0.4607, -1.5955, -0.4461,  0.5013, -0.4416]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 5\n",
    "batch_size = 3\n",
    "element_dimensions = 6\n",
    "\n",
    "#Create a tensor of a batch of 3 sequences, of length 5, where each element of the sequence is a vector of 6 dimensions (often determined by the word embedding)\n",
    "#NOTE: So in this case, for each time step, we will have a batch of 3 sequences, where each sequence is a vector of 6 dimensions \n",
    "random_tensor = torch.randn(sequence_length, batch_size,element_dimensions)\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Understanding embedding layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1213, -1.0444, -0.0745],\n",
       "         [-0.1213, -1.0444, -0.0745],\n",
       "         [ 1.7691, -0.1360, -0.4914],\n",
       "         [-1.0926, -1.0567,  0.1238]],\n",
       "\n",
       "        [[ 1.7691, -0.1360, -0.4914],\n",
       "         [ 0.2484, -0.3891,  1.8460],\n",
       "         [-0.3511, -0.7464, -0.9632],\n",
       "         [-0.5328,  0.4752, -0.6882]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Playing around with the embedding layer\n",
    "embedding = torch.nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1, 1, 4, 5], [4, 3, 2, 9]])\n",
    "\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Understanding LSTM layers\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Playing around with the lstm layer\n",
    "rnn = torch.nn.LSTM(input_size=10, hidden_size=20, num_layers=2)\n",
    "input = torch.randn(5, 3,10) # sequence length of 5, batch size of 3, 10 features --> This represents 3 sequences consiting of 5 time steps, where each element is a 10-dimensional vector\n",
    "h0 = torch.randn(2, 3, 20) #initial hidden state --> 2 layers (one hidden state for each layer of the LSTM, since this example has 2 LSTM layers), batch size of 3, so we need 3 vectors per batch, each vector will be 20 dimensions \n",
    "c0 = torch.randn(2, 3, 20) #initial cell state --> Size follows the same logic as above \n",
    "\n",
    "#NOTE: DIMENSIONS OF HIDDEN STATE AND CELL STATE MUST BE THE SAME\n",
    "\n",
    "output, (hn, cn) = rnn(input, (h0, c0)) #output gives the hiden state for all time steps of the last layer of the LSTM, hn gives the hidden state for the last time step of the last layer of the LSTM, cn gives the cell state for the last time step of the last layer of the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 20])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = torch.nn.Linear(20,1) #Params: input size , output size (in this case, the output size is 1 for a regressoion task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_reshaped = output.view(-1, 20) #Reshapes the output as: (sequence length  batch size, hidden size)\n",
    "linear_output = linear_layer(output_reshaped) #Applies the linear layer to the reshaped output\n",
    "final_output = linear_output.view(5, 3, 1) #Output is reshaped back to the original shape --> For each time step, we have a batch of 3 sequences, where each sequence is now a vector of 1 dimension (representing the predicted output for that sequence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0236],\n",
       "         [-0.0412],\n",
       "         [ 0.0109]],\n",
       "\n",
       "        [[ 0.0293],\n",
       "         [-0.0037],\n",
       "         [ 0.0025]],\n",
       "\n",
       "        [[ 0.0150],\n",
       "         [-0.0058],\n",
       "         [-0.0040]],\n",
       "\n",
       "        [[ 0.0074],\n",
       "         [-0.0198],\n",
       "         [-0.0004]],\n",
       "\n",
       "        [[-0.0017],\n",
       "         [-0.0255],\n",
       "         [ 0.0048]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic of stacked lSTMS (for example 2 LSTMS)\n",
    "\n",
    "    The output of the first LSTM layer at each timestep is passed as input to the second LSTM layer for the same timestep.\n",
    "    The hidden state (htht) from the second LSTM layer's computation for one timestep is then used in the second LSTM's computation for the next timestep.\n",
    "\n",
    "This happens for each timestep in the sequence, so the second LSTM processes the sequence concurrently with the first LSTM but relies on the outputs from the first LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### When batch_first = true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When batch_first = true, the output of the LSTM will now be of the shape (batch_size, sequence_length, hidden_size)\n",
    "#Playing around with the lstm layer\n",
    "rnn = torch.nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n",
    "input = torch.randn(3, 5,10) #Now we must swap these dimensions around so batch size is first\n",
    "h0 = torch.randn(2, 3, 20) #initial hidden state --> 2 layers (one hidden state for each layer of the LSTM, since this example has 2 LSTM layers), batch size of 3, so we need 3 vectors per batch, each vector will be 20 dimensions \n",
    "c0 = torch.randn(2, 3, 20) #initial cell state --> Size follows the same logic as above \n",
    "\n",
    "# #NOTE: DIMENSIONS OF HIDDEN STATE AND CELL STATE MUST BE THE SAME\n",
    "\n",
    "output, (hn, cn) = rnn(input, (h0, c0)) #output gives the hiden state for all time steps of the last layer of the LSTM, hn gives the hidden state for the last time step of the last layer of the LSTM, cn gives the cell state for the last time step of the last layer of the LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 20])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #This tensor has a batch of size 3, meaning there are 3 sequences in this batch. Each sequence has 5 time steps, where each time step is a 10-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = torch.nn.Linear(20,1) \n",
    "#output_reshaped = output.reshape(-1, 20) #Reshapes the output as: (sequence length  batch size, hidden size)\n",
    "linear_output = linear_layer(output) #Applies the linear layer to the reshaped output\n",
    "final_output = linear_output.view(3, 5, 1)  # Shape: (batch size, sequence length, output_size)\n",
    "final_output\n",
    "\n",
    "#Each batch now has a sequence of 5 elements, where each eleemnt is a scalar value (output of the linear layer) for that time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Understanding reshaping of output and target tensors for loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1040676, 15]) Target shape: torch.Size([1040676])\n"
     ]
    }
   ],
   "source": [
    "# auto_reg_model = autoregressive_model(embedding_size=32, vocab_size=len(w2i_ndfa), num_layers_lstm=1, hidden_size_lstm=16)\n",
    "auto_reg_model = elman_model(embedding_size=300, vocab_size=len(w2i_ndfa), hidden_size=300)\n",
    "\n",
    "test_input = batches_dictionary_padded[(0, 10)]\n",
    "\n",
    "output = auto_reg_model.forward(test_input)\n",
    "\n",
    "output_reshaped = output.reshape(-1, len(w2i_ndfa)) #Reshape the output tensor to have the shape (batch_size*sequence_length, vocab_size)\n",
    "\n",
    "target = batches_dictionary_target_tensors[(0, 10)] \n",
    "\n",
    "target_reshaped = target.reshape(-1) #Reshape the target tensor to have the shape (batch_size*sequence_length)\n",
    "\n",
    "print(\"Output shape:\",output_reshaped.shape, \"Target shape:\", target_reshaped.shape)\n",
    "\n",
    "#NOTE:\n",
    "#------------------------\n",
    "#Target: Provides the ground truth index of the next element in the sequence.\n",
    "#Output: Contains logits (unnormalized scores) for all possible values of the next element at each timestep.\n",
    "#The model learns to align its output logits with the target indices by minimizing the cross-entropy loss.\n",
    "\n",
    "#The reshaped output merges all the sequences from all the batches into a single batch of sequences, where each sequence gives the logits for the next element in the sequence. \n",
    "# The reshaped target tensor contains the ground truth indices of the next element in the sequence for each timestep in the batch.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
