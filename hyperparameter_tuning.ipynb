{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module for optimization\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "# module for logging data \n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "# module for retriving datat \n",
    "from bayes_opt.util import load_logs\n",
    "\n",
    "# import the function to be optimized\n",
    "from main3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Elman params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pbounds = {\"batch_size\" : (32, 128), \"learning_rate\" : (0.001, 0.009), \"num_epochs\": (1, 5)}\n",
    "def train_elman_rnn_wrapper(batch_size, learning_rate, num_epochs):\n",
    "    nn_type = \"elman_torch\" # elman_scratch, elman_torch, lstm, mlp\n",
    "    batch_size = int(round(batch_size))\n",
    "    num_epochs = int(round(num_epochs))\n",
    "    epoch_num, train_loss, val_accuracy = train_rnn(batch_size, learning_rate, num_epochs, nn_type)\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer1 = BayesianOptimization(\n",
    "    f = train_elman_rnn_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer1.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning1.log\")\n",
    "optimizer1.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer1.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer1.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer1.max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning LSTM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|‚ñè         | 4/278 [00:14<16:43,  3.66s/it, loss=0.691]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m optimizer2\u001b[38;5;241m.\u001b[39msubscribe(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP, logger)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# initial search \u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43moptimizer2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# number of random explorations before bayes_opt\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# number of bayes_opt iterations\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# print out the data from the initial run to check if bounds need update \u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(optimizer2\u001b[38;5;241m.\u001b[39mres):\n",
      "File \u001b[1;32mc:\\Users\\skhur\\miniconda3\\envs\\dl\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:311\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    309\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest(util)\n\u001b[0;32m    310\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[1;32mc:\\Users\\skhur\\miniconda3\\envs\\dl\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39madd(params)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[1;32mc:\\Users\\skhur\\miniconda3\\envs\\dl\\Lib\\site-packages\\bayes_opt\\target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    234\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_as_array(params)\n\u001b[0;32m    235\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keys, x))\n\u001b[1;32m--> 236\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtrain_lstm_rnn_wrapper\u001b[1;34m(batch_size, learning_rate, num_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(batch_size))\n\u001b[0;32m      7\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(num_epochs))\n\u001b[1;32m----> 8\u001b[0m epoch_num, train_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_accuracy\n",
      "File \u001b[1;32mc:\\Users\\skhur\\Desktop\\VU Amsterdam\\Nov-Dec 2024\\Deep Learning\\Assignments\\Assignment3\\DeepLearning-VU-\\main3.py:277\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[1;34m(batch_size, learning_rate, num_epochs, nn_type)\u001b[0m\n\u001b[0;32m    274\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 277\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\skhur\\Desktop\\VU Amsterdam\\Nov-Dec 2024\\Deep Learning\\Assignments\\Assignment3\\DeepLearning-VU-\\main3.py:189\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[0;32m    187\u001b[0m batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    188\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, num_classes)\u001b[39;00m\n\u001b[0;32m    190\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, batch_y)\n\u001b[0;32m    191\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\skhur\\miniconda3\\envs\\dl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\skhur\\miniconda3\\envs\\dl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\skhur\\Desktop\\VU Amsterdam\\Nov-Dec 2024\\Deep Learning\\Assignments\\Assignment3\\DeepLearning-VU-\\main3.py:110\u001b[0m, in \u001b[0;36mlstm_RNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03mForward pass:\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03mx: (batch, time)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mPool over time dimension to get (batch, num_classes)\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)  \u001b[38;5;66;03m# (batch, time, emb_dim)\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m o_l2, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, time, hidden_dim)\u001b[39;00m\n\u001b[0;32m    111\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(o_l2)  \u001b[38;5;66;03m# (batch, time, num_classes)\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Global max pooling over time dimension\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\skhur\\miniconda3\\envs\\dl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\skhur\\miniconda3\\envs\\dl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\skhur\\miniconda3\\envs\\dl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# bounded parameter regions \n",
    "pbounds = {\"batch_size\" : (32, 128), \"learning_rate\" : (0.001, 0.009), \"num_epochs\": (1, 5)}\n",
    "# define wrapped funciton\n",
    "def train_lstm_rnn_wrapper(batch_size, learning_rate, num_epochs):\n",
    "    nn_type = \"lstm\" # elman_scratch, elman_torch, lstm, mlp\n",
    "    batch_size = int(round(batch_size))\n",
    "    num_epochs = int(round(num_epochs))\n",
    "    epoch_num, train_loss, val_accuracy = train_rnn(batch_size, learning_rate, num_epochs, nn_type)\n",
    "    return val_accuracy\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer2 = BayesianOptimization(\n",
    "    f = train_lstm_rnn_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer2.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning2.log\")\n",
    "optimizer2.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer2.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer2.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer2.max)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning MLP params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounded parameter regions \n",
    "pbounds = {\"batch_size\" : (32, 128), \"learning_rate\" : (0.001, 0.009), \"num_epochs\": (1, 5)}\n",
    "\n",
    "# define wrapped funciton\n",
    "def train_mlp_rnn_wrapper(batch_size, learning_rate, num_epochs):\n",
    "    nn_type = \"mlp\" # elman_scratch, elman_torch, lstm, mlp\n",
    "    batch_size = int(round(batch_size))\n",
    "    num_epochs = int(round(num_epochs))\n",
    "    epoch_num, train_loss, val_accuracy = train_rnn(batch_size, learning_rate, num_epochs, nn_type)\n",
    "    return val_accuracy\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer3 = BayesianOptimization(\n",
    "    f = train_mlp_rnn_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer3.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning3.log\")\n",
    "optimizer3.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer3.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer3.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer3.max)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
